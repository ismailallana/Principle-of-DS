{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Decision trees\n",
    "\n",
    "[Adapted from http://scikit-learn.org/stable/modules/tree.html]\n",
    "\n",
    "Let's try a decision tree on Iris data.\n",
    "\n",
    "### Train and view a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "key=', '.join(['{}={}'.format(i,name) for i,name in enumerate(iris.target_names)])\n",
    "\n",
    "# First let's create a train and test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.33,\n",
    "                                                    random_state=5) # so we get the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Let's fit a model\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "_ = tree.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate\n",
    "print('Classification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html\n",
    "def get_code(tree, feature_names, target_names, spacer_base=\"    \"):\n",
    "    \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-leant DescisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    target_names -- list of target (class) names.\n",
    "    spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    based on http://stackoverflow.com/a/30104792.\n",
    "    \"\"\"\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    def recurse(left, right, threshold, features, node, depth):\n",
    "        spacer = spacer_base * depth\n",
    "        if (threshold[node] != -2):\n",
    "            print(spacer + \"if ( \" + features[node] + \" <= \" + \\\n",
    "                  str(threshold[node]) + \" ) {\")\n",
    "            if left[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            left[node], depth+1)\n",
    "            print(spacer + \"}\\n\" + spacer +\"else {\")\n",
    "            if right[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            right[node], depth+1)\n",
    "            print(spacer + \"}\")\n",
    "        else:\n",
    "            target = value[node]\n",
    "            for i, v in zip(np.nonzero(target)[1],\n",
    "                            target[np.nonzero(target)]):\n",
    "                target_name = target_names[i]\n",
    "                target_count = int(v)\n",
    "                print(spacer + \"return \" + str(target_name) + \\\n",
    "                      \" ( \" + str(target_count) + \" examples )\")\n",
    "\n",
    "    recurse(left, right, threshold, features, 0, 0)\n",
    "    \n",
    "print('Decision tree:\\n')\n",
    "get_code(tree, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Model selection on test data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McNemar's test\n",
    "\n",
    "McNemar's test is [recommended when we have a single test split](http://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf).\n",
    "\n",
    "Under H0, the two algorithms should have the same error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def mcnemar(x, y):\n",
    "    n1 = np.sum(x < y)\n",
    "    n2 = np.sum(x > y)\n",
    "    stat = (np.abs(n1-n2)-1)**2 / (n1+n2)\n",
    "    df = 1\n",
    "    pval = chi2.sf(stat,1)\n",
    "    return stat, pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Compare classifiers\n",
    "\n",
    "- Choose the decision tree max_depth in [2..6], criterion in ['entropy', 'gini'] and splitter in ['best', 'random']. What are the best parameters? Print out all grid scores to sanity check the selection. Is there a unique best set of parameters?\n",
    "- Use `np.array` create `l_yn` and `t_yn` arrays showing respectively for logistic regression and decision tree whether each test instance is predicted correctly (`1`) or incorrectly (`0`). Are the classifiers significantly different at p<=0.05 according to McNemar's test?(use the logistic regression code from previous week)\n",
    "- Which classifier is significantly better at p<=0.05 using paired t-test?(use f-score measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Perform grid search\n",
    "param_grid = [\n",
    "    {'max_depth': [2, 3, 4, 5, 6],\n",
    "     'criterion': ['entropy', 'gini'],\n",
    "     'splitter': ['best', 'random']}\n",
    "]\n",
    "tree = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "tree.fit(X_train, Y_train)\n",
    "\n",
    "# Print grid search results\n",
    "print('Grid search mean and stdev:\\n')\n",
    "scoring = tree.cv_results_\n",
    "for mean_score, std, params in zip(scoring['mean_test_score'],scoring['std_test_score'],scoring['params']):\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "            mean_score, std * 2, params))\n",
    "    \n",
    "#for params, mean_score, scores in tree.cv_results_:\n",
    "   # print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "       #     mean_score, scores.std() * 2, params))\n",
    "\n",
    "# Print best params\n",
    "print('\\nBest parameters:', tree.best_params_)\n",
    "\n",
    "# Evaluate on held-out test\n",
    "print('\\nClassification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "_ = logreg.fit(X_train, Y_train)\n",
    "# Calculate whether each test prediction is correct\n",
    "l_yn = np.array([int(p==t) for p,t in zip(logreg.predict(X_test), Y_test)])\n",
    "t_yn = np.array([int(p==t) for p,t in zip(tree.predict(X_test), Y_test)])\n",
    "\n",
    "# There's very little difference in this data set\n",
    "print(l_yn)\n",
    "print(t_yn)\n",
    "\n",
    "# We cannot reject H0. Accuracy is different but not reliably so.\n",
    "# Therefore, we can select either classifier, e.g.,\n",
    "# decision tree for interpretability.\n",
    "print('\\nCan we reject H0?', 'Yes' if mcnemar(l_yn, t_yn)[1]<0.05 else 'No')\n",
    "\n",
    "print('Classification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From data to decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Ensembling classifiers\n",
    "\n",
    "### Load and split data\n",
    "\n",
    "scikit-learn provides a `train_test_split` function (in `sklearn.cross_validation`). However, there is no function to do a three-way split into training, development and held-out test data.\n",
    "\n",
    "Let's create a three-way 50/25/25 train/dev/test split by using `train_test_split` two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# load digits data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_td, X_test, y_td, y_test = train_test_split(digits.data, digits.target, test_size=0.25,\n",
    "                                              random_state=5) # so we get the same results\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_td, y_td, test_size=0.33,\n",
    "                                                  random_state=5) # so we get the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot error vs complexity for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "NUM_SAMPLES = 10\n",
    "NUM_TRAIN_SETS = 10\n",
    "\n",
    "def subsample(X, y, sample_size):\n",
    "    xy_tuples = list(zip(X, y))\n",
    "    xy_sample = [random.choice(xy_tuples) for _ in range(sample_size)]\n",
    "    X_sample, y_sample = zip(*xy_sample)\n",
    "    return X_sample, y_sample\n",
    "\n",
    "def error(clf, X, y):\n",
    "    \"Calculate error as 1-accuracy\"\n",
    "    return 1-clf.score(X,y)\n",
    "\n",
    "def bootstrap_error(clf, X_train, y_train, X_test, y_test, sample_size, num_samples=NUM_SAMPLES):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for _ in range(num_samples):\n",
    "        X_sample, y_sample = subsample(X_train, y_train, sample_size)\n",
    "        clf.fit(X_sample, y_sample)\n",
    "        train_errors.append(error(clf,X_sample,y_sample))\n",
    "        test_errors.append(error(clf,X_test,y_test))\n",
    "    train_error = sum(train_errors)/len(train_errors)\n",
    "    test_error = sum(test_errors)/len(test_errors)\n",
    "    return train_error, test_error\n",
    "\n",
    "complexities = []\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for max_depth in [2,4,8,16,32,None]:\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    sample_size = len(y_train)\n",
    "    train_error, test_error = bootstrap_error(clf, X_train, y_train, X_dev, y_dev, sample_size)\n",
    "    complexities.append(max_depth)\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "plt.plot(complexities, train_errors, c='b', label='Training error')\n",
    "plt.plot(complexities, test_errors, c='r', label='Generalisation error')\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Model complexity')\n",
    "plt.title('Decision tree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Assessing decision tree fit\n",
    "\n",
    "- Does training or generalisation error level out first? Why?\n",
    "- What is the best value of max_depth based on this plot?\n",
    "- Why doesn't generalisation error increase on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Generalisation error levels out first.\n",
    "#     This suggests that higher values of max_depth may lead to overfitting.\n",
    "\n",
    "# 2 - max_depth=8. This gives the best generalisation error with lower \n",
    "#     model complexity and less risk of overfitting.\n",
    "\n",
    "# 3 - The algorithm has other mechanisms to prevent overfitting.\n",
    "#     And overfitting does seem to hurt generalisation too much on this data.\n",
    "#     Nevertheless, decision trees can overfit so use with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot error vs complexity for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "complexities = []\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for n_estimators in [1,2,4,8,16,32,64]:\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=8)\n",
    "    sample_size = len(y_train)\n",
    "    train_error, test_error = bootstrap_error(clf, X_train, y_train, X_dev, y_dev, sample_size)\n",
    "    complexities.append(n_estimators)\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "plt.plot(complexities, train_errors, c='b', label='Training error')\n",
    "plt.plot(complexities, test_errors, c='r', label='Generalisation error')\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Model complexity')\n",
    "plt.title('Random forest')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot error vs number of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def plot_error_curves(clf, X_train, y_train, X_test, y_test, num_train_sets=NUM_TRAIN_SETS, title=None):\n",
    "    data_sizes = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for i in range(num_train_sets):\n",
    "        sample_size = int(len(y_train) * (i+1)/num_train_sets)\n",
    "        train_error, test_error = bootstrap_error(clf, X_train, y_train, X_test, y_test, sample_size)\n",
    "        data_sizes.append(sample_size)\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "    plt.plot(data_sizes, train_errors, c='b', label='Training error')\n",
    "    plt.plot(data_sizes, test_errors, c='r', label='Generalisation error')\n",
    "    plt.ylim(0,1)\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Number of training samples')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Note that we're passing dev data for estimating generalisation error here, not test data\n",
    "dt = DecisionTreeClassifier(max_depth=8)\n",
    "plot_error_curves(dt, X_train, y_train, X_dev, y_dev, title='Decision Tree')\n",
    "rf = RandomForestClassifier(max_depth=8, n_estimators=16)\n",
    "plot_error_curves(rf, X_train, y_train, X_dev, y_dev, title='Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Comparing fit and data needed\n",
    "\n",
    "- Which classifier would you use?\n",
    "- Would it be useful to collect more training data?\n",
    "- The decision tree has a larger spread between training and generalisation error. Why is this?\n",
    "- Note we haven't yet used test data. When is it OK to use the held-out test data from our train/dev/test split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Random forest is clearly better (error rates under 10% vs 20% for decision tree)\n",
    "\n",
    "# 2 - Yes, almost always. However, it looks like both classifiers are close to their asymptotes.\n",
    "#     So the benefit might not be worth the cost.\n",
    "#     The decision tree would benefit more from additional data.\n",
    "\n",
    "# 3 - The decision tree suffers more from overfitting.\n",
    "#     The random forest on this particular data has 0 training error.\n",
    "#     This is a bit of a surprise as random forests tend to increas bias.\n",
    "#     With high bias, we would expect underfitting which tends to be \n",
    "#     characterised by both high training and high generalisation error.\n",
    "#     However, random forests generally also reduce variance enough\n",
    "#     to cancel out any increase in bias.\n",
    "#     Here we end up with a nice generalisation error plot that seems\n",
    "#     to be close to its asymptote and not too different from the\n",
    "#     training error.\n",
    "\n",
    "# 4 - As little as possible. Ideally only once for our final \n",
    "#     generalisation error/accuracy calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
