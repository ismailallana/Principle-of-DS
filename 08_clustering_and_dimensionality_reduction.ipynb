{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering  and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create example data for KMeans clustering from scratch\n",
    "First, let's create some example data with 4 clusters using make_blobs.\n",
    "\n",
    "We set random_state=1 so we all get the same clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distict cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)  # For reproducibility\n",
    "d1 = X[:,0] # first dimension\n",
    "d2 = X[:,1] # second dimension\n",
    "plt.scatter(d1,d2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now write a function that initializes k centroids by randomly selecting them from the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(points, k):\n",
    "    \"\"\"returns k centroids from the initial points\"\"\"\n",
    "    centroids = points.copy()\n",
    "    np.random.shuffle(centroids)\n",
    "    return centroids[:k]\n",
    "\n",
    "k = 4;\n",
    "\n",
    "centroids = initialize_centroids(X, k)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now write a function that initializes k centroids following kmeans++ approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "def initialize_centroids_plus_plus(X, K):\n",
    "    C = [X[0]]\n",
    "    indexes = [0]\n",
    "    for k in range(1, K):\n",
    "        D2 = scipy.array([min([scipy.inner(c-x,c-x) for c in C]) for x in X])\n",
    "        probs = D2/D2.sum()\n",
    "        cumprobs = probs.cumsum()\n",
    "        r = scipy.rand()\n",
    "        for j,p in enumerate(cumprobs):\n",
    "            if r < p:\n",
    "                i = j\n",
    "                break\n",
    "        C.append(X[i])\n",
    "        indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "k = 4;\n",
    "indexes = initialize_centroids_plus_plus(X, k)\n",
    "centroids = X[indexes]\n",
    "print(centroids)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's define a function that returns the closest centroid for each point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_centroid(points, centroids):\n",
    "    \"\"\"returns an array containing the index to the nearest centroid for each point\"\"\"\n",
    "    distances = np.sqrt(((points - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "    return np.argmin(distances, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can test the code like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = initialize_centroids(X, k)\n",
    "closest_centroid(X, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The last step in the algorithm is to move the centroids to the mean location associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_centroids(points, closest, centroids):\n",
    "    \"\"\"returns the new centroids assigned from the points closest to them\"\"\"\n",
    "    return np.array([points[closest==k].mean(axis=0) for k in range(centroids.shape[0])])\n",
    "\n",
    "move_centroids(X, closest_centroid(X, c), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can visualize the first two steps in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "indexes = initialize_centroids_plus_plus(X, k)\n",
    "centroids = X[indexes]\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', s=100)\n",
    "plt.show()\n",
    "\n",
    "closest = closest_centroid(X, centroids)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=closest)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', s=100)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "closest = closest_centroid(X, centroids)\n",
    "centroids = move_centroids(X, closest, centroids)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=closest)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', s=100)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's define a function that returns the distnace between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance Caculator\n",
    "def dist(a, b):\n",
    "    return np.sqrt(((a - b)**2).sum())\n",
    "    #return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The complete algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4;\n",
    "indexes = initialize_centroids_plus_plus(X, k)\n",
    "centroids = X[indexes]\n",
    "# To store the value of centroids when it updates\n",
    "centroids_old = np.zeros(centroids.shape)\n",
    "\n",
    "error = dist(centroids, centroids_old)\n",
    "print(\"Error: \",error)\n",
    "\n",
    "\n",
    "# Loop will run till the error becomes zero\n",
    "while error != 0:\n",
    "    # Storing the old centroid values\n",
    "    centroids_old = centroids\n",
    "    # Assigning each value to its closest cluster\n",
    "    closest = closest_centroid(X, centroids)\n",
    "    # Finding the new centroids by taking the average value\n",
    "    centroids = move_centroids(X, closest, centroids)\n",
    "    # update the error\n",
    "    error = dist(centroids, centroids_old)  \n",
    "    print(\"Error: \", error)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=closest)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans using sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "estimator = KMeans(init='k-means++', n_clusters=4, n_init=10)\n",
    "estimator.fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(estimator.cluster_centers_[:, 0], estimator.cluster_centers_[:, 1], c='r', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Clustering with k-means\n",
    "\n",
    "[Adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#example-cluster-plot-kmeans-digits-py]\n",
    "\n",
    "### Loading handwritten digits data\n",
    "\n",
    "We'll work with the handwritten digits dataset, a classic machine-learning dataset used to explore automatic recognition of handwritten digits (i.e., 0, 1, 2, ..., 9).\n",
    "\n",
    "For more information:\n",
    "* http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits\n",
    "* http://scikit-learn.org/stable/tutorial/basic/tutorial.html#loading-an-example-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print('The digits data comprises {} {}-dimensional representations of handwritten digits:\\n{}\\n'.format(\n",
    "        digits.data.shape[0],\n",
    "        digits.data.shape[1],\n",
    "        digits.data\n",
    "    ))\n",
    "\n",
    "print('It also includes labels:\\n{}\\n'.format(digits.target))\n",
    "\n",
    "print('And it includes the original 8x8 image representation:\\n{}\\n'.format(digits.images[0]))\n",
    "\n",
    "print('Let\\'s look at a few images:\\n')\n",
    "NUM_SUBPLOT_ROWS = 1\n",
    "NUM_SUBPLOT_COLS = 8\n",
    "for i in range(NUM_SUBPLOT_ROWS*NUM_SUBPLOT_COLS):\n",
    "    _ = plt.subplot(NUM_SUBPLOT_ROWS,NUM_SUBPLOT_COLS,i+1)\n",
    "    _ = plt.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering handwritten digits\n",
    "\n",
    "That's the data. Now let's try clustering these 64d vectors.\n",
    "\n",
    "`scikit-learn` implements many differnt machine learning algorithms.\n",
    "\n",
    "The normal pattern is to:\n",
    "1. intialise an estimator (e.g., `estimator = KMeans()`)\n",
    "1. fit to the training data (e.g., `estimator.fit(training_data)`)\n",
    "1. label the test data (e.g., `estimator.predict(test_data)`)\n",
    "\n",
    "For clustering, we don't have separate training and test data.\n",
    "\n",
    "So the labelling is created when we fit and accessed by `estimator.labels_`.\n",
    "\n",
    "`estimator.inertia_` gives the sum of squared errors (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# First let's scale the digits data (center to mean and scale to unit variance)\n",
    "data = scale(digits.data)\n",
    "print('Scaled digits data:\\n{}\\n'.format(data))\n",
    "\n",
    "# Let's grab the data we'll need\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target)) # classes\n",
    "labels = digits.target\n",
    "freq = (Counter(labels))\n",
    "print ('Label counts for each digit:',sorted(freq.items()))\n",
    "\n",
    "# And let's run k-means, specifying initialisation (k-means++), k (n_digits),\n",
    "# and the number of runs (10)\n",
    "estimator = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "estimator.fit(data)\n",
    "print('Sum of squared errors:', estimator.inertia_)\n",
    "print('Clusters from k-means:', estimator.labels_[:10])\n",
    "print('Gold standard classes:', labels[:10])\n",
    "\n",
    "y_actu = pd.Series(labels, name='Actual')\n",
    "y_pred = pd.Series(estimator.labels_, name='Predicted')\n",
    "confusion = pd.crosstab(y_pred, y_actu)\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Clusters from k-means:', estimator.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Try different initialisations\n",
    "\n",
    "Initialisation has a large effect on cluster output. Let's try a few options.\n",
    "\n",
    "* Initialise with random (`init='random'`)\n",
    "* Run PCA with k components (`pca = PCA(n_components=n_digits).fit(data)`)\n",
    "* Use PCA components to initialise (`init=pca.components_`)\n",
    "* Can we determine which approach is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - \n",
    "est_random = KMeans(init='random', n_clusters=n_digits, n_init=10)\n",
    "est_random.fit(data)\n",
    "print('RANDOM INITIALISATION')\n",
    "print('Num of squared errors:', est_random.inertia_)\n",
    "print('Clusters from k-means:', est_random.labels_[:10])\n",
    "print('Gold standard classes:', labels[:10])\n",
    "print('')\n",
    "\n",
    "# 2 - \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "\n",
    "# 3 - \n",
    "est_pca = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)\n",
    "est_pca.fit(data)\n",
    "print('INITIALISATION WITH PCA COMPONENTS')\n",
    "print('Num of squared errors:', est_pca.inertia_)\n",
    "print('Clusters from k-means:', est_pca.labels_[:10])\n",
    "print('Gold standard classes:', labels[:10])\n",
    "print('')\n",
    "\n",
    "# 4 - It looks like k-means++ >> random >> PCA from SSE/inertia.\n",
    "#     But SSE is an internal validation measure.\n",
    "#     Since we're trying to cluster by digit, we can't really say\n",
    "#     which is best without comparing to the gold partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Evaluating clustering\n",
    "\n",
    "Since we have a gold-standard labels, we can compare our system clustering to the true partition.\n",
    "\n",
    "`scikit-learn` includes various metrics for this:\n",
    "* Homogeneity\n",
    "* Completeness\n",
    "* V-measure\n",
    "* Adjusted Rand index (ARI)\n",
    "* Adjusted mutual information (AMI)\n",
    "* Silhouette coefficient\n",
    "\n",
    "For more information:\n",
    "* http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation\n",
    "\n",
    "Let's compare the above clusterings using V-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('k-means++ initialisation:', metrics.v_measure_score(labels, estimator.labels_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing initialisations\n",
    "\n",
    "Let's be a bit more exhastive, comparing initialisations using variuos evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    \"Calculate various metrics for comparing system clustering to a gold partition\"\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "# print table header\n",
    "print(75 * '_')\n",
    "print('init         time  inertia    homo   compl  v-meas     ARI     AMI silhouet')\n",
    "print(75 * '_')\n",
    "\n",
    "# benchmark k-means++ initialisation\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "# benchmark random initialisation\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# benchmark PCA initalisation\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "\n",
    "print(75 * '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Reading evaluation output\n",
    "\n",
    "- Which approach performs best? How would you order the other two?\n",
    "- Do you neighbours get the same result?\n",
    "- Can we apply statistical significance testing?\n",
    "- How else can we test reliability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - PCA > random, PCA > k-means++, hard to say for random and k-means++\n",
    "\n",
    "# 2 - Run multiple times, k-means clustering depends on initialisation and changes\n",
    "\n",
    "# 3 - Not directly to the clustering output. Clustering is often more of an exploratory tool.\n",
    "\n",
    "# 4 - A few possibilities..\n",
    "#     We could evaluate the impact of different clusterings on another task.\n",
    "#     We could do a bootstrap stability analysis\n",
    "#     (http://www.r-bloggers.com/bootstrap-evaluation-of-clusters/).\n",
    "#     We could use cophenitic correlation for hierarchical clustering\n",
    "#     (https://en.wikipedia.org/wiki/Cophenetic_correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Choosing k\n",
    "\n",
    "### Create example data for choosing k\n",
    "\n",
    "First, let's create some example data with 4 clusters using make_blobs.\n",
    "\n",
    "We set `random_state=1` so we all get the same clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distict cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)  # For reproducibility\n",
    "d1 = X[:,0] # first dimension\n",
    "d2 = X[:,1] # second dimension\n",
    "_ = plt.scatter(d1,d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k using silhouette analysis\n",
    "\n",
    "[Adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html]\n",
    "\n",
    "For good clusterings:\n",
    "* the average silhouette should be close to 1 indicating that points are far away from neighbouring clusters \n",
    "* all cluster silhouettes should be close to the average silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "range_n_clusters = range(2,6)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.Spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhoutte score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Choosing k\n",
    "\n",
    "[Derived from Data Science from Scratch, Chapter 19]\n",
    "\n",
    "- Plot inertia against k for k from 2 to 6\n",
    "- What k would you choose for each? Is it the same?\n",
    "- Does this work on the handwritten digits code? Why / why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 -\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "inertia_values = [KMeans(n_clusters=k).fit(X).inertia_ for k in range_n_clusters]\n",
    "_ = plt.plot(range_n_clusters, inertia_values)\n",
    "\n",
    "# 2 - From this plot, it looks like the knee is at 4 or maybe 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3 - Nope. The handwritten digits data is difficult to cluster.\n",
    "#     However, we haven't done anything clever with our feature representation.\n",
    "#     We might do better, e.g., with spectral clustering\n",
    "#     (http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering).\n",
    "#     Q: Does spectral clustering outperform \n",
    "#     We leave this as an extra exercise for the keen reader.\n",
    "range_n_clusters = range(5,15)\n",
    "inertia_values = [KMeans(n_clusters=k).fit(data).inertia_ for k in range_n_clusters]\n",
    "_ = plt.plot(range_n_clusters, inertia_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EXERCISE: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "print('The iris data comprises {} {}-dimensional representations of Iris flower.'.format(\n",
    "        iris.data.shape[0],\n",
    "        iris.data.shape[1]\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.DataFrame(data= np.c_[iris ['data'], iris ['target']],\n",
    "                     columns= iris ['feature_names'] + ['target'])\n",
    "\n",
    "df_iris['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the data we'll need\n",
    "n_classes = len(np.unique(iris.target_names)) # classes\n",
    "print(\"Number of classes = \",n_classes)\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "#Plot the initial training points using sepal length vs sepal width\n",
    "\n",
    "# Plot the training points\n",
    "def plot_scatter(X,y,labels,colors):\n",
    "    for i in range(len(colors)):\n",
    "        px = X[:, 0][y == i]\n",
    "        py = X[:, 1][y == i]\n",
    "        plt.scatter(px, py, c=colors[i])\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.show()\n",
    "\n",
    "print('Let\\'s see how the data looks like:\\n')\n",
    "colors = ['black', 'blue', 'purple']  \n",
    "\n",
    "plot_scatter(iris.data, iris.target,iris.target_names, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply PCA on the data and reduce its dimensionality\n",
    "pca = PCA()\n",
    "data = scale(iris.data)\n",
    "iris_pca = pca.fit_transform(data)\n",
    "#Explained variance ratio\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3))\n",
    "print(var)\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.plot(var)\n",
    "plt.show()\n",
    "#Cumulative variance\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding how many componenets\n",
    "The output of the explained varinace shows  that the first component explains 72.8%  of the data set's variation. That means it holds 72.8% of the data's information in one principal component. And by taking the first two components, we only exlude 4.2% of the data set's information and the first two components  contain 95.8% of the iris data set's original information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2)\n",
    "data = scale(iris.data)\n",
    "iris_pca = pca.fit_transform(data)\n",
    "comps = pd.DataFrame(data = iris_pca, columns = ['pc 1', 'pc 2'])\n",
    "comps[1:5]\n",
    "\n",
    "final_comps = pd.concat([comps, pd.DataFrame(data=labels,columns=['target'])], axis = 1)\n",
    "final_comps[1:5]\n",
    "#print top five records\n",
    "final_comps.iloc[1:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "def plot_pca_scatter(X,y,labels,colors):\n",
    "    for i in range(len(colors)):\n",
    "        px = X[:, 0][y == i]\n",
    "        py = X[:, 1][y == i]\n",
    "        plt.scatter(px, py, c=colors[i])\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points after PCA\n",
    "colors = ['black', 'blue', 'purple']  \n",
    "plot_pca_scatter(iris_pca, iris.target,iris.target_names, colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO PCA on digits dataset\n",
    "* Load digits dataset \n",
    "* Scale the data\n",
    "* Apply PCA\n",
    "* What would be a reasonable number of components for digits  data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "#raise NotImplementedError\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print('The digits data comprises {} {}-dimensional representations of handwritten digits:\\n{}\\n'.format(\n",
    "        digits.data.shape[0],\n",
    "        digits.data.shape[1],\n",
    "        digits.data\n",
    "    ))\n",
    "\n",
    "print('It also includes labels:\\n{}\\n'.format(digits.target))\n",
    "\n",
    "print('And it includes the original 8x8 image representation:\\n{}\\n'.format(digits.images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Let\\'s look at a few images:\\n')\n",
    "NUM_SUBPLOT_ROWS = 1\n",
    "NUM_SUBPLOT_COLS = 8\n",
    "for i in range(NUM_SUBPLOT_ROWS*NUM_SUBPLOT_COLS):\n",
    "    plt.subplot(NUM_SUBPLOT_ROWS,NUM_SUBPLOT_COLS,i+1)\n",
    "    plt.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's scale the digits data (center to mean and scale to unit variance)\n",
    "data = scale(digits.data)\n",
    "print('Scaled digits data:\\n{}\\n'.format(data))\n",
    "\n",
    "# Let's grab the data we'll need\n",
    "n_samples, n_features = data.shape\n",
    "print(\"Number of dimensions = \",n_features)\n",
    "n_digits = len(np.unique(digits.target)) # classes\n",
    "labels = digits.target\n",
    "print(digits.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "digits_pca = pca.fit_transform(data)\n",
    "#Explained variance ratio\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3))\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.plot(var)\n",
    "plt.show()\n",
    "#Cumulative variance\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=40)\n",
    "digits_pca = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "def plot_pca_scatter(X_pca,y_digits,labels,colors):\n",
    "    for i in range(len(colors)):\n",
    "        px = X_pca[:, 0][y_digits == i]\n",
    "        py = X_pca[:, 1][y_digits == i]\n",
    "        plt.scatter(px, py, c=colors[i])\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()\n",
    "\n",
    "colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']\n",
    "   \n",
    "plot_pca_scatter(digits_pca, labels,digits.target_names, colors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
